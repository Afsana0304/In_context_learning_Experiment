------------------- code setup :rocket:-----------------------------
1. Code for experimenting with hugging face data : code/hugging_face_data_experiment.py
2. Code for experimenting with new synthetic generated data Gpt : code/Gpt_data_experiment.py
3. Code for experimenting with new synthetic generated data using Deepseek: code/deep_seek_data_experiment.py
4. Code for experimenting with hugging face data but only 1 example: code/memory_eff_backup.py
5. In the cloud setup the model and the weights were downloaded and they were being used from the chache dir.


-------------------------- Dataset :rocket:-------------------------
1. For experiment 1 the data was taken from this hugging face directory: https://huggingface.co/datasets/abisee/cnn_dailymail
2. For Experiments 2 and 3, the synthetic data used is stored in the synthetic_data folder. The datasets are available in CSV format: data1.csv contains the GPT-generated data, and data2.csv contains the DeepSeek-generated data.

-------------------------- plots :rocket:-------------------------
1. The result for the experimenting with hugging faces 100 data is available on: Plots of all Experiment 

(epoch_i_precision_scores.png = for i incontext example how the rouge score precision is changing for the 100 test data(from hugging face) 
(average rouge_scores vs the number of examples.png = Our final result :star: After increasing the number of incontext example how the average rouge precesion score of the 100 test data is chnaging)

------------------------ slurm file :rocket: ----------------------

submit_job.sh has the all necessary setup to put a job in the cloud. The runtime is set to 11h and 40Gb GPU memory is being requested.

Note: For interactive job (For debugging purpose)
Use salloc command:

for accessing gx21 use: salloc -A demelo -p sorcery --gpus=1 --mem=80G
for accessing gx01-gx05 use: salloc -A demelo -p sorcery --gres=gpu:a100:1 --mem=80G

-------------------- Some basic command in the cloud :fire:-----------------
1. squeue -u afsana.mimi (shows the job that is running on the cloud along with the node)
2. scontrol show job <job_id> (shows more details including when the submitted slurm file will start running)
3. scancel -u afsana.mimi (cancels all the jobs at once running under the user name)
4. scancel -u <job_id> (cancels only a particular job)

------------------------ slurm file output :rocket: ----------------------

For each experiment, there is a corresponding SLURM output file. These files contain detailed logs for every test example processed, including the generated summary and its ROUGE score against the ground truth. This information is valuable for reviewing the quality of the generated summaries, identifying any potential issues, and troubleshooting areas that may require improvement. The SLURM output files are located in the "output of each experiment" folder

------------------------ Generated vs Ground Truth Summaries :fire: ----------------------

In the reference_vs_generated_summary/ folder, the model-generated summaries and the corresponding ground truth summaries from the Hugging Face dataset are stored in a flexible CSV format. This format allows for easy inspection and comparison of the summaries.
The following columns can be found in each CSV file:

Epoch: The epoch number is indicated, representing how many in-context examples were used during that epoch. For example, Epoch 1 means that only 1 in-context example was used, while Epoch 2 indicates 2 in-context examples, and so on.

Generated Summary: The summary generated by the model for each test example.

Ground Truth Summary: The reference summary from the Hugging Face dataset, used as the "ideal" summary for comparison.


------------------ Environment creatrion :fire: --------------------
1. use python3 -m venv icl for creating a new envrionment called icl
2. pip install -r requirements.txt
3. For activing the environment use : source icl/bin/activate
4. For deactivating : deactivate





   
